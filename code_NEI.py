# -*- coding: utf-8 -*-
"""A3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q7X88A28RVkXP6EVqgG6vehUJrBl9E59
"""

!pip install datasets

from datasets import load_dataset
dataset = load_dataset('conllpp')

import pandas as pd
import numpy as np

features = ['token', 'pos_tag', 'cw1', 'cw2', 'cw3', 'cw4', 'cw5', 'cw6', 'pt1', 'pt2', 'pt3', 'pt4', 'is_capital', 'wlen', 'is_firstw', 'is_prevne']
df = pd.DataFrame(columns=(features + ['target']))
vocab = list(set([item for sublist in (dataset['train']['tokens'] + dataset['validation']['tokens'])for item in sublist]))
vocab.append('unk')
vocab = {vocab[i] : i + 1 for i in range(len(vocab))}
vocab['0'] = 0
sentences = dataset['train']['tokens'] + dataset['validation']['tokens']
pos_tags = dataset['train']['pos_tags'] + dataset['validation']['pos_tags']
ner_tags = dataset['train']['ner_tags'] + dataset['validation']['ner_tags']
train = []
for i in range(len(sentences)):
  sent = ['0'] + sentences[i] + ['0']
  pos = [0] + pos_tags[i] + [0]
  ner = [0] + ner_tags[i] + [0]
  prev_ner = 0
  for j in range(1, len(sent) - 1):
    if(j == 1):
      is_firstw = 1
    else :
      is_firstw = 0
    if(ner[j] > 0):
      ner_tag = 1
    else:
      ner_tag = 0
    if(len(sent[j]) > 0 and sent[j][0] >= 'A' and sent[j][0] <= 'Z'):
      is_capital = 1
    else :
      is_capital = 0
    l = {'token' : vocab[sent[j]], 'pos_tag' : pos[j],
         'cw1' : vocab[sent[max(0, j - 3)]], 'cw2' : vocab[sent[max(0, j - 2)]], 'cw3' : vocab[sent[max(0, j - 1)]], 'cw4' : vocab[sent[min(len(sent) - 1, j + 1)]], 'cw5' : vocab[sent[min(len(sent) - 1, j + 2)]], 'cw6' : vocab[sent[min(len(sent) - 1, j + 3)]], 
         'pt1' : pos[max(0, j - 2)], 'pt2' : pos[max(0, j - 1)], 'pt3' : pos[min(len(pos) - 1, j + 1)], 'pt4' : pos[min(len(pos) - 1, j + 2)], 
         'is_capital' : is_capital, 'wlen' : len(sent[j]), 'is_firstw' : is_firstw, 'is_prevne' : prev_ner, 'target' : ner_tag }
    train.append(l)
    prev_ner = ner_tag

df = df.append(train, ignore_index = True)
print(df)

test_df = pd.DataFrame(columns=(features + ['target']))
sentences = dataset['test']['tokens']
pos_tags = dataset['test']['pos_tags']
ner_tags = dataset['test']['ner_tags']
test = []
prev_ner = 0
for i in range(len(sentences)):
  sent = ['0'] + sentences[i] + ['0']
  pos = [0] + pos_tags[i] + [0]
  ner = [0] + ner_tags[i] + [0]
  for j in range(1, len(sent) - 1):
    if(j == 1):
      is_firstw = 1
    else :
      is_firstw = 0
    if(ner[j] > 0):
      ner_tag = 1
    else:
      ner_tag = 0
    if(len(sent[j]) > 0 and sent[j][0] >= 'A' and sent[j][0] <= 'Z'):
      is_capital = 1
    else :
      is_capital = 0
    l = {'token' : vocab.get(sent[j], vocab['unk']), 'pos_tag' : pos[j],
         'cw1' : vocab.get(sent[max(0, j - 3)], vocab['unk']), 'cw2' : vocab.get(sent[max(0, j - 2)], vocab['unk']), 'cw3' : vocab.get(sent[max(0, j - 1)], vocab['unk']), 'cw4' : vocab.get(sent[min(len(sent) - 1, j + 1)], vocab['unk']), 'cw5' : vocab.get(sent[min(len(sent) - 1, j + 2)], vocab['unk']), 'cw6' : vocab.get(sent[min(len(sent) - 1, j + 3)], vocab['unk']), 
         'pt1' : pos[max(0, j - 2)], 'pt2' : pos[max(0, j - 1)], 'pt3' : pos[min(len(pos) - 1, j + 1)], 'pt4' : pos[min(len(pos) - 1, j + 2)], 
         'is_capital' : is_capital, 'wlen' : len(sent[j]), 'is_firstw' : is_firstw, 'is_prevne' : prev_ner, 'target' : ner_tag }
    test.append(l)

test_df = test_df.append(test, ignore_index = True)
print(test_df)

"""# Normal"""

from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler

clf = SVC(C=1.0, kernel='rbf')
X = np.array(df[features])
scaler = StandardScaler()
X = scaler.fit_transform(X)
y = np.array(df['target'])
y = y.astype('int')
clf.fit(X, y)

from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import classification_report, precision_recall_fscore_support

X_test = np.array(test_df[features])
X_test = scaler.transform(X_test)
y_test = np.array(test_df['target'])
y_test = y_test.astype('int')
y_pred = clf.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
print(accuracy_score(y_test, y_pred))
print(precision_recall_fscore_support(y_test, y_pred, average='weighted'))
print(precision_recall_fscore_support(y_test, y_pred, average='weighted', beta=0.5))
print(precision_recall_fscore_support(y_test, y_pred, average='weighted', beta=2))

import pickle
filename = '/content/drive/MyDrive/CS626/clf_svc_normal.sav'
pickle.dump(clf, open(filename, 'wb'))

clf = pickle.load(open('/content/drive/MyDrive/CS626/clf_svc_normal.sav', 'rb'))
sentence = test_df.loc[0:10]['token']
for k, v in vocab.items():
  if(v in sentence):
    print(k)
y_true = y_test[0:10]
y_pred = clf.predict(X_test[0:10])
# print(sentence)
print("y", y_true, y_pred)

"""## Undersampling"""

from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from imblearn.under_sampling import RandomUnderSampler
from collections import Counter

undersample = RandomUnderSampler(sampling_strategy='majority')
clf = SVC()
X = np.array(df[features])
scaler = StandardScaler()
X = scaler.fit_transform(X)
y = np.array(df['target'])
y = y.astype('int')
print("Before undersampling: ", Counter(y))
X, y = undersample.fit_resample(X, y)
print("After undersampling: ", Counter(y))
clf.fit(X, y)

from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import classification_report, precision_recall_fscore_support

X_test = np.array(test_df[features])
X_test = scaler.transform(X_test)
y_test = np.array(test_df['target'])
y_test = y_test.astype('int')
y_pred = clf.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
print(accuracy_score(y_test, y_pred))

import pickle
filename = '/content/drive/MyDrive/CS626/clf_svc_us.sav'
pickle.dump(clf, open(filename, 'wb'))

"""# Oversampling"""

from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from collections import Counter

SMOTE = SMOTE()
clf = SVC()
X = np.array(df[['token', 'pos_tag', 'cw1', 'cw2', 'cw3', 'cw4', 'cw5', 'cw6', 'pt1', 'pt2', 'pt3', 'pt4', 'is_capital', 'is_len', 'is_firstw', 'is_prevne']])
scaler = StandardScaler()
X = scaler.fit_transform(X)
y = np.array(df['target'])
y = y.astype('int')
print("Before oversampling: ", Counter(y))
X, y = SMOTE.fit_resample(X, y)
clf.fit(X, y)

from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import classification_report, precision_recall_fscore_support

X_test = np.array(test_df[['token', 'pos_tag', 'cw1', 'cw2', 'cw3', 'cw4', 'cw5', 'cw6', 'pt1', 'pt2', 'pt3', 'pt4', 'is_capital', 'is_len', 'is_firstw', 'is_prevne']])
X_test = scaler.transform(X_test)
y_test = np.array(test_df['target'])
y_test = y_test.astype('int')
y_pred = clf.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
print(accuracy_score(y_test, y_pred))

import pickle
filename = '/content/drive/MyDrive/CS626/clf_svc_os.sav'
pickle.dump(clf, open(filename, 'wb'))

"""# Combination of both"""

from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline
from collections import Counter
from sklearn.model_selection import cross_val_score
from numpy import mean

SMOTE = SMOTE()
clf = SVC()
X = np.array(df[['token', 'pos_tag', 'cw1', 'cw2', 'cw3', 'cw4', 'cw5', 'cw6', 'pt1', 'pt2', 'pt3', 'pt4', 'is_capital', 'is_len', 'is_firstw', 'is_prevne']])
scaler = StandardScaler()
X = scaler.fit_transform(X)
y = np.array(df['target'])
y = y.astype('int')
over = SMOTE(sampling_strategy=0.4)
under = RandomUnderSampler(sampling_strategy=0.5)
steps = [('o', over), ('u', under), ('model', clf)]
pipeline = Pipeline(steps=steps)
scores = cross_val_score(pipeline, X, y, scoring='accuracy_score', cv=5, n_jobs=-1)
score = mean(scores)
print('Accuracy Score : ', score)

from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import classification_report, precision_recall_fscore_support

X_test = np.array(test_df[['token', 'pos_tag', 'cw1', 'cw2', 'cw3', 'cw4', 'cw5', 'cw6', 'pt1', 'pt2', 'pt3', 'pt4', 'is_capital', 'is_len', 'is_firstw', 'is_prevne']])
X_test = scaler.transform(X_test)
y_test = np.array(test_df['target'])
y_test = y_test.astype('int')
y_pred = clf.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
print(accuracy_score(y_test, y_pred))

import pickle
filename = '/content/drive/MyDrive/CS626/clf_svc_pipelined.sav'
pickle.dump(clf, open(filename, 'wb'))